input: # files to use
    prompt: [/home/stefanop/alidock/DmesonAnalysis/ML/parquet_file/parquet_file_ITS2/training_set/Prompt_STE__pT_1_50.parquet.gzip]
    FD: [/home/stefanop/alidock/DmesonAnalysis/ML/parquet_file/parquet_file_ITS2/training_set/FD_STE__pT_1_50.parquet.gzip]
    data: [/home/stefanop/alidock/DmesonAnalysis/ML/parquet_file/parquet_file_ITS2/training_set/Bkg_STE__pT_1_50.parquet.gzip]

output:
    leg_labels: ['Background', 'Prompt D$_s$', 'Feed-down D$_s$'] # legend labels
    out_labels: ['Bkg', 'Prompt', 'FD'] # output labels
    dir: '/home/stefanop/alidock/DmesonAnalysis/ML/AnalysisDs/MLoutput_ITS3/ML_output_w_optimisation' # output dir

pt_ranges: # ranges in pt to split the data in the ml training and testing
    min: [1, 2, 4, 8, 12, 16] # list
    max: [2, 4, 8, 12, 16, 24] # list

data_prep:
    filt_bkg_mass: '' # pandas query to select bkg candidates, if '' is not applied  
    dataset_opt: equal # change how the dataset is built, options available: 'equal', 'max_signal'
                       # 'equal' -> same number of prompt/FD/bkg (not using all the signal available)
                       # 'max_signal' -> try to use all the signal (prompt and FD) + add n_bkg = bkg_mult * (n_prompt + n_FD)
    bkg_mult: [2.] # list of multipliers for (nPrompt + nFD) used to determine nCandBkg in the 'max_signal' option
    seed_split: 42 # seed used for train_test_split(...)
    test_fraction: 1 # fraction of data used for test set and efficiencies --> set to 1. if you want to apply the model to the full dataframes  
    
ml:
    raw_output: False # use raw_output (True) of probability (False) as output of the model
    roc_auc_average: 'macro' # 'macro' or 'weighted'
    roc_auc_approach: 'ovo'  # 'ovo' or 'ovr'
    training_columns: ['inv_mass', 'pt_cand', 'd_len', d_len_xy, norm_dl_xy, cos_p, cos_p_xy, imp_par_xy, sig_vert, max_norm_d0d0exp, nsigComb_Pi_0, nsigComb_K_0, nsigComb_Pi_1, nsigComb_K_1, nsigComb_Pi_2, nsigComb_K_2, delta_mass_KK, dca]
                       # list of training variables

    hyper_par: [{'max_depth':4, 'learning_rate':0.1, 'n_estimators':800, 'min_child_weight':2, 'subsample':1., 'colsample_bytree':0.9, n_jobs':1, 'tree_method':hist},
                {'max_depth':4, 'learning_rate':0.1, 'n_estimators':800, 'min_child_weight':2, 'subsample':1., 'colsample_bytree':0.9, n_jobs':1, 'tree_method':hist},
                {'max_depth':4, 'learning_rate':0.1, 'n_estimators':800, 'min_child_weight':2, 'subsample':1., 'colsample_bytree':0.9, n_jobs':1, 'tree_method':hist},
                {'max_depth':4, 'learning_rate':0.1, 'n_estimators':800, 'min_child_weight':2, 'subsample':1., 'colsample_bytree':0.9, n_jobs':1, 'tree_method':hist},
                {'max_depth':4, 'learning_rate':0.1, 'n_estimators':800, 'min_child_weight':2, 'subsample':1., 'colsample_bytree':0.9, n_jobs':1, 'tree_method':hist},
                {'max_depth':4, 'learning_rate':0.1, 'n_estimators':800, 'min_child_weight':2, 'subsample':1., 'colsample_bytree':0.9, n_jobs':1, 'tree_method':hist}]
                # list of dicts of hyperparameters (one for each pT bin)

    hyper_par_opt:
        do_hyp_opt: True # whether to do the parameter optimization
        njobs: 6 # number of parallel jobs used in hyper-parameter optimization, -1. to use all
        nfolds: 5 # number of folds used in cross validation
        initpoints: 10 # steps of random exploration you want to perform
        niter: 15 # steps for bayesian optimization
        bayes_opt_config: {'max_depth': !!python/tuple [3, 8], 
                           'learning_rate': !!python/tuple [0.01, 0.1],
                           'n_estimators': !!python/tuple [300, 1500], 
                           'min_child_weight': !!python/tuple [1, 10],
                           'subsample': !!python/tuple [0.8, 1.], 
                           'colsample_bytree': !!python/tuple [0.8, 1.]}
                        # configuration dictionary for optimize_params_bayes()

    saved_models: [] 
                   # list of saved ModelHandler (path+file), compatible with the pt bins

plots:
    plotting_columns: [inv_mass, pt_cand, cos_p_xy, d_len, norm_dl_xy, sig_vert, delta_mass_KK, imp_par_xy,
                       max_norm_d0d0exp, nsigComb_Pi_0, nsigComb_Pi_1, nsigComb_Pi_2, nsigComb_K_0, nsigComb_K_1, nsigComb_K_2] 
                       # list of variables to plot
    train_test_log: True # use log scale for plots of train and test distributions

df_column_to_save_opt: 
    df_column_to_save_list: ['inv_mass', 'pt_cand', 'dca', 'imp_par_xy'] # list of variables saved in the dataframes with the applied models
