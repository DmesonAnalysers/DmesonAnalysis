input: # files to use, set FD to null for binary classification
    prompt: ~/DmesonAnalysis/filterdata/filtered/Prompt_filter_pT_0_50.parquet.gzip
    FD: ~/DmesonAnalysis/filterdata/filtered/FD_filter_pT_0_50.parquet.gzip
    data: ~/DmesonAnalysis/filterdata/filtered/Data_filter_pT_0_50.parquet.gzip

output:
    leg_labels: ["Background", "Prompt L$_c$", "Feed-down L$_c$"] # legend labels, keep the right number of classes
    out_labels: ["Bkg", "Prompt", "FD"] # output labels, keep the right number of classes
    dir: ~/DmesonAnalysis/ML/trained_models/LctopK0s/5TeV/ # output dir

pt_ranges: # ranges in pt to split the data in the ml training and testing
    min: [2, 4, 8] # list
    max: [4, 8, 50] # list

data_prep:
    filt_bkg_mass: inv_mass < 1.832 or inv_mass > 2.012 # pandas query to select bkg candidates
    dataset_opt:
        'max_signal' # change how the dataset is built, options available: 'equal', 'max_signal'
        # 'equal' -> same number of prompt/FD/bkg (not using all the signal available)
        # 'max_signal' -> try to use all the signal (prompt and FD) + add n_bkg = bkg_mult * (n_prompt + n_FD)
    bkg_mult: [0.3, 0.1, 0.05] # list of multipliers for (nPrompt + nFD) used to determine nCandBkg in the 'max_signal' option
    seed_split: 42 # seed used for train_test_split(...)
    test_fraction: 0.2 # fraction of data used for test set and efficiencies --> set to 1. if you want to apply the model to the full dataframes

ml:
    raw_output: False # use raw_output (True) or probability (False) as output of the model
    roc_auc_average: "macro" # 'macro' or 'weighted'
    roc_auc_approach: "ovo" # 'ovo' or 'ovr'
    training_columns:
        [
            cos_p,
            cos_p_xy,
            d_len,
            d_len_xy,
            norm_dl_xy,
            imp_par_xy,
            delta_mass_K0s,
            nsigComb_Pi_0,
            nsigComb_Pr_0,
        ]
        # list of training variables

    hyper_par:
        [
            {
                "max_depth": 4,
                "learning_rate": 0.029,
                "n_estimators": 665,
                "min_child_weight": 2.7,
                "subsample": 0.90,
                "colsample_bytree": 0.97,
                "n_jobs": 4,
                "tree_method": hist,
            },
            {
                "max_depth": 4,
                "learning_rate": 0.092,
                "n_estimators": 674,
                "min_child_weight": 7.0,
                "subsample": 0.90,
                "colsample_bytree": 0.81,
                "n_jobs": 4,
                "tree_method": hist,
            },
            {
                "max_depth": 4,
                "learning_rate": 0.092,
                "n_estimators": 674,
                "min_child_weight": 7.0,
                "subsample": 0.90,
                "colsample_bytree": 0.81,
                "n_jobs": 4,
                "tree_method": hist,
            },
            {
                "max_depth": 4,
                "learning_rate": 0.070,
                "n_estimators": 672,
                "min_child_weight": 8.7,
                "subsample": 0.87,
                "colsample_bytree": 0.82,
                "n_jobs": 4,
                "tree_method": hist,
            },
            {
                "max_depth": 3,
                "learning_rate": 0.086,
                "n_estimators": 352,
                "min_child_weight": 8.6,
                "subsample": 0.96,
                "colsample_bytree": 0.95,
                "n_jobs": 2,
                "tree_method": hist,
            },
        ]
        # list of dicts of hyperparameters (one for each pT bin)

    hyper_par_opt:
        do_hyp_opt: False # whether to do the parameter optimization
        njobs: 8 # number of parallel jobs used in hyper-parameter optimization, -1. to use all
        nfolds: 5 # number of folds used in cross validation
        initpoints: 10 # steps of random exploration you want to perform
        niter: 15 # steps for bayesian optimization
        bayes_opt_config:
            {
                "max_depth": !!python/tuple [2, 4],
                "learning_rate": !!python/tuple [0.01, 0.1],
                "n_estimators": !!python/tuple [200, 1000],
                "min_child_weight": !!python/tuple [1, 10],
                "subsample": !!python/tuple [0.8, 1.],
                "colsample_bytree": !!python/tuple [0.8, 1.],
            }
            # configuration dictionary for optimize_params_bayes()

    saved_models:
        [
            ~/DmesonAnalysis/ML/trained_models/LctopK0s/ModelHandler_pT_2_4.pickle,
            ~/DmesonAnalysis/ML/trained_models/LctopK0s/ModelHandler_pT_4_8.pickle,
            ~/DmesonAnalysis/ML/trained_models/LctopK0s/ModelHandler_pT_8_50.pickle,
        ]
        # list of saved ModelHandler (path+file), compatible with the pt bins

plots:
    plotting_columns:
        [
            inv_mass,
            pt_cand,
            cos_p,
            cos_p_xy,
            d_len,
            d_len_xy,
            norm_dl_xy,
            imp_par_xy,
            delta_mass_K0s,
            nsigComb_Pi_0,
            nsigComb_Pr_0,
        ]
        # list of variables to plot
    train_test_log: True # use log scale for plots of train and test distributions

appl:
    column_to_save_list: ["inv_mass", "pt_cand"] # list of variables saved in the dataframes with the applied models

standalone_appl:
    inputs: [] # list of parquet files for the model application
    output_names: [] # names for the outputs (one for each file)
    output_dir: null # output directory
