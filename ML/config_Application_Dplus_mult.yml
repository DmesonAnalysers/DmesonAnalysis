input: # files to use, set FD to null for binary classification
    prompt: ../../AnalysisNonPromptDpp2017/Dplus/MC/LHC20a7/Prompt_Dpluspp5TeV_LHC20a7_pT_1_50.parquet.gzip
    FD: ../../AnalysisNonPromptDpp2017/Dplus/MC/LHC20a7/FD_Dpluspp5TeV_LHC20a7_pT_1_50.parquet.gzip
    data: ../../AnalysisNonPromptDpp2017/Dplus/Data/LHC17pq/Data_Dpluspp5TeV_LHC17pq_pT_1_50.parquet.gzip

output:
    leg_labels: ['Background', 'Prompt D$^+$', 'Feed-down D$^+$'] # legend labels, keep the right number of classes
    out_labels: ['Bkg', 'Prompt', 'FD'] # output labels, keep the right number of classes
    dir: '../../AnalysisNonPromptDpp2017/Dplus/outputs/ML' # output dir

pt_ranges: # ranges in pt to split the data in the ml training and testing
    min: [1, 2, 3, 4, 5, 6, 8, 10, 12, 24] # list
    max: [2, 3, 4, 5, 6, 8, 10, 12, 24, 36] # list

data_prep:
    filt_bkg_mass: inv_mass < 1.82 or 1.92 < inv_mass < 2.00 # pandas query to select bkg candidates
    dataset_opt: equal  # change how the dataset is built, options available: 'equal', 'max_signal'
                        # 'equal' -> same number of prompt/FD/bkg (not using all the signal available)
                        # 'max_signal' -> try to use all the signal (prompt and FD) + add n_bkg = 2 * (n_prompt + n_FD)
    bkg_mult: [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.] # list of multipliers for (nPrompt + nFD) used to determine nCandBkg in the 'max_signal' option
    seed_split: 42 # seed used for train_test_split(...)
    test_fraction: 1. # fraction of data used for test set and efficiencies --> set to 1. if you want to apply the model to the full dataframes  

ml:
    raw_output: False # use raw_output (True) of probability (False) as output of the model
    roc_auc_average: 'macro' # 'macro' or 'weighted'
    roc_auc_approach: 'ovo'  # 'ovo' or 'ovr'
    training_columns: [d_len, norm_dl_xy, cos_p, cos_p_xy, imp_par_xy, sig_vert, max_norm_d0d0exp, 
                       nsigComb_Pi_0, nsigComb_K_0, nsigComb_Pi_1, nsigComb_K_1, nsigComb_Pi_2, nsigComb_K_2] 
                       # list of training variables

    hyper_par: [{'max_depth':3, 'learning_rate':0.0238, 'n_estimators':1028, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
                {'max_depth':4, 'learning_rate':0.0210, 'n_estimators':1282, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
                {'max_depth':4, 'learning_rate':0.0210, 'n_estimators':1282, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
                {'max_depth':4, 'learning_rate':0.0210, 'n_estimators':1282, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
                {'max_depth':4, 'learning_rate':0.0210, 'n_estimators':1282, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
                {'max_depth':4, 'learning_rate':0.0210, 'n_estimators':1282, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
                {'max_depth':3, 'learning_rate':0.0283, 'n_estimators':1067, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
                {'max_depth':3, 'learning_rate':0.0283, 'n_estimators':1067, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
                {'max_depth':3, 'learning_rate':0.0261, 'n_estimators':1041, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist},
                {'max_depth':2, 'learning_rate':0.0261, 'n_estimators':500, 'min_child_weight':5, 'colsample':0.9, 'n_jobs':4, 'tree_method':hist}]
                # list of dicts of hyperparameters (one for each pT bin)

    hyper_par_opt:
      do_hyp_opt: False # whether to do the parameter optimization
      njobs: 2 # number of parallel jobs used in hyper-parameter optimization, -1. to use all
      nfolds: 5 # number of folds used in cross validation
      initpoints: 5 # steps of random exploration you want to perform
      niter: 5 # steps for bayesian optimization
      bayes_opt_config: {'max_depth': !!python/tuple [2, 6], 
                        'learning_rate': !!python/tuple [0.02, 0.1]}
                        # configuration dictionary for optimize_params_bayes()
    
    saved_models: [../../AnalysisNonPromptDpp2017/Dplus/outputs/ML/pt1_2/ModelHandler_pT_1_2.pickle, 
                   ../../AnalysisNonPromptDpp2017/Dplus/outputs/ML/pt2_3/ModelHandler_pT_2_3.pickle, 
                   ../../AnalysisNonPromptDpp2017/Dplus/outputs/ML/pt3_4/ModelHandler_pT_3_4.pickle, 
                   ../../AnalysisNonPromptDpp2017/Dplus/outputs/ML/pt4_5/ModelHandler_pT_4_5.pickle, 
                   ../../AnalysisNonPromptDpp2017/Dplus/outputs/ML/pt5_6/ModelHandler_pT_5_6.pickle, 
                   ../../AnalysisNonPromptDpp2017/Dplus/outputs/ML/pt6_8/ModelHandler_pT_6_8.pickle, 
                   ../../AnalysisNonPromptDpp2017/Dplus/outputs/ML/pt8_10/ModelHandler_pT_8_10.pickle, 
                   ../../AnalysisNonPromptDpp2017/Dplus/outputs/ML/pt10_12/ModelHandler_pT_10_12.pickle, 
                   ../../AnalysisNonPromptDpp2017/Dplus/outputs/ML/pt12_24/ModelHandler_pT_12_24.pickle,
                   ../../AnalysisNonPromptDpp2017/Dplus/outputs/ML/pt24_36/ModelHandler_pT_24_36.pickle] # list of saved ModelHandler (path+file), compatible with the pt bins

plots:
    plotting_columns: [inv_mass, pt_cand, d_len, norm_dl_xy, cos_p, cos_p_xy, imp_par_xy, sig_vert, 
                       max_norm_d0d0exp, nsigComb_Pi_0, nsigComb_K_0, nsigComb_Pi_1, nsigComb_K_1, nsigComb_Pi_2, nsigComb_K_2] 
                       # list of variables to plot
    train_test_log: True # use log scale for plots of train and test distributions
  
df_column_to_save_opt: 
    df_column_to_save_list: ['inv_mass', 'pt_cand'] # list of variables saved in the dataframes with the applied models
