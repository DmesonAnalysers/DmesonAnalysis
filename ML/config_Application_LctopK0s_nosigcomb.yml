input: # files to use, set FD to null for binary classification
    prompt:
        [
            /data/dbattistini/filtered/Prompt_filter_pT_2_4.root,
            /data/dbattistini/filtered/Prompt_filter_pT_4_6.root,
            /data/dbattistini/filtered/Prompt_filter_pT_6_8.root,
            /data/dbattistini/filtered/Prompt_filter_pT_8_12.root,
        ]
    FD:
        [
            /data/dbattistini/filtered/FD_filter_pT_2_4.root,
            /data/dbattistini/filtered/FD_filter_pT_4_6.root,
            /data/dbattistini/filtered/FD_filter_pT_6_8.root,
            /data/dbattistini/filtered/FD_filter_pT_8_12.root,
        ]
    data:
        [
            /data/dbattistini/filtered/Data_filter_pT_2_4.root,
            /data/dbattistini/filtered/Data_filter_pT_4_6.root,
            /data/dbattistini/filtered/Data_filter_pT_6_8.root,
            /data/dbattistini/filtered/Data_filter_pT_8_12.root,
        ]
    treename: treeMLLc

output:
    leg_labels: # legend labels, keep the right number of classes
        Bkg: Background
        Prompt: Prompt $\Lambda_c^+$
        FD: Feed-down $\Lambda_c^+$
    out_labels: # output labels, keep the right number of classes
        Bkg: Bkg
        Prompt: Prompt
        FD: FD
    dir: /home/dbattistini/DmesonAnalysis/ML/trained_models/LctopK0s/nosigcomb # output dir

pt_ranges: # ranges in pt to split the data in the ml training and testing
    min: [2, 4, 6, 8] # list
    max: [4, 6, 8, 12] # list

data_prep:
    filt_bkg_mass: inv_mass < 2.21 or inv_mass > 2.36 # pandas query to select bkg candidates
    dataset_opt:
        "max_signal" # change how the dataset is built, options available: 'equal', 'max_signal'
        # 'equal' -> same number of prompt/FD/bkg (not using all the signal available)
        # 'max_signal' -> try to use all the signal (prompt and FD) + add n_bkg = bkg_mult * (n_prompt + n_FD)
    # bkg_mult: [1, 0.3, 0.1] # list of multipliers for (nPrompt + nFD) used to determine nCandBkg in the 'max_signal' option
    bkg_mult: [1, 0.3, 0.1, 0.05] # list of multipliers for (nPrompt + nFD) used to determine nCandBkg in the 'max_signal' option
    seed_split: 42 # seed used for train_test_split(...)
    test_fraction: 1 # fraction of data used for test set and efficiencies --> set to 1. if you want to apply the model to the full dataframes

ml:
    raw_output: False # use raw_output (True) or probability (False) as output of the model
    roc_auc_average: "macro" # 'macro' or 'weighted'
    roc_auc_approach: "ovo" # 'ovo' or 'ovr'
    training_columns:
        []
        # list of training variables

    hyper_par:
        []
        # list of dicts of hyperparameters (one for each pT bin)

    hyper_par_opt:
        do_hyp_opt: False # whether to do the parameter optimization
        njobs: 5 # number of parallel jobs used in hyper-parameter optimization, -1. to use all
        nfolds: 5 # number of folds used in cross validation
        initpoints: 10 # steps of random exploration you want to perform
        niter: 15 # steps for bayesian optimization
        bayes_opt_config:
            {
                "max_depth": !!python/tuple [5, 9],
                "learning_rate": !!python/tuple [0.005, 0.1],
                "n_estimators": !!python/tuple [500, 1200],
                "min_child_weight": !!python/tuple [0.2, 7],
                "subsample": !!python/tuple [0.8, 1.],
                "colsample_bytree": !!python/tuple [0.8, 1.],
            }
            # configuration dictionary for optimize_params_bayes()

    saved_models: [
            /home/dbattistini/DmesonAnalysis/ML/trained_models/LctopK0s/nosigcomb/pt2_4/ModelHandler_pT_2_4.pickle,
            /home/dbattistini/DmesonAnalysis/ML/trained_models/LctopK0s/nosigcomb/pt4_6/ModelHandler_pT_4_6.pickle,
            /home/dbattistini/DmesonAnalysis/ML/trained_models/LctopK0s/nosigcomb/pt6_8/ModelHandler_pT_6_50.pickle,
            /home/dbattistini/DmesonAnalysis/ML/trained_models/LctopK0s/nosigcomb/pt8_12/ModelHandler_pT_8_12.pickle,
        ] # list of saved ModelHandler (path+file), compatible with the pt bins
plots:
    plotting_columns: [] # list of variables to plot
    train_test_log: True # use log scale for plots of train and test distributions

appl:
    column_to_save_list: ["inv_mass", "pt_cand"] # list of variables saved in the dataframes with the applied models

standalone_appl:
    inputs: [] # list of parquet files for the model application
    output_names: [] # names for the outputs (one for each file)
    output_dir: null # output directory
